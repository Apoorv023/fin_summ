{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGrMR4BYhBOHpuSJBP9m+t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install required libraries**\n",
        "\n",
        "pip install sec-edgar-downloader sec-api beautifulsoup4 langchain-core langchain-text-splitters langchain-community qdrant-client pandas"
      ],
      "metadata": {
        "id": "vg7diycUooRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sec-edgar-downloader sec-api beautifulsoup4 langchain-core langchain-text-splitters langchain-community qdrant-client pandas sentence-transformers transformers torch accelerate"
      ],
      "metadata": {
        "id": "NILTRI6AownW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create directory to store**"
      ],
      "metadata": {
        "id": "hvyxGfp8wUOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "folder_name = \"sec_filings\"\n",
        "os.makedirs(folder_name, exist_ok=True)"
      ],
      "metadata": {
        "id": "uo0Bl1ZCwTWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download all the required data**\n",
        "\n",
        "Format link: https://sec-api.io/resources/extract-googles-revenue-metrics-from-10-k-filings-with-python"
      ],
      "metadata": {
        "id": "We2UhkOuovvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sec_edgar_downloader import Downloader\n",
        "\n",
        "# Replace this with your actual name and email as SEC requires a user-agent string\n",
        "USER_AGENT_COMPANY_NAME = \"XYZ_COMPANY_NAME\"\n",
        "USER_AGENT_EMAIL = \"work.email@mail.com\"\n",
        "\n",
        "# Initialize the downloader\n",
        "dl = Downloader(USER_AGENT_COMPANY_NAME, USER_AGENT_EMAIL, download_folder=\"sec_filings\")\n",
        "\n",
        "# List of tickers (sample tickers provided ‚Äî replace or expand)\n",
        "tickers = {\"AAPL\" : \"Apple\", \"MSFT\" : \"Microsoft\", \"AMZN\" : \"Amazon\", \"GOOGL\" : \"Google\",\n",
        "           \"META\" : \"Meta\", \"BRK-B\" : \"Berkshire\", \"JNJ\" : \"Johnson\", \"V\" : \"Visa\",\n",
        "           \"PG\" : \"ProcterGamble\", \"TSLA\" : \"Tesla\"}\n",
        "\n",
        "# Download the last 4 years' 10-K filings for each ticker\n",
        "for ticker in tickers:\n",
        "    print(f\"Downloading 10-K filings for {ticker}...\")\n",
        "    try:\n",
        "        dl.get(\"10-K\", ticker, limit=4)  # last 4 filings (usually 1 per year)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {ticker}: {e}\")\n",
        "\n",
        "print(\"Download completed.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VCzIg5JnKUOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change the extension of txt files to html**"
      ],
      "metadata": {
        "id": "UU39QjzH66U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base_folder = r\"/content/sec_filings\"\n",
        "for root, _, file in os.walk(base_folder):\n",
        "        if file and file[0].lower().endswith(\".txt\"):\n",
        "            new_file_name = os.path.splitext(file[0])[0] + \".html\" # rename txt file to html file\n",
        "            old_file_name = os.path.join(root, file[0])\n",
        "            new_file_name = os.path.join(root, new_file_name)\n",
        "            try:\n",
        "                os.rename(old_file_name, new_file_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Error caught in {root}\")\n",
        "\n",
        "print(\"File extensions were changed sucessfully!\")"
      ],
      "metadata": {
        "id": "FzPZXPuUK0Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the financial data and store it in csv files**"
      ],
      "metadata": {
        "id": "6HkqW0Bz7CR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sec_api import XbrlApi\n",
        "from google.colab import userdata\n",
        "\n",
        "SEC_API_KEY = userdata.get('SEC_API_KEY')\n",
        "xbrlApi = XbrlApi(SEC_API_KEY)\n",
        "\n",
        "def get_income_statement(xbrl_json):\n",
        "    income_statement_store = {}\n",
        "\n",
        "    # iterate over each US GAAP item in the income statement\n",
        "    for usGaapItem in xbrl_json['StatementsOfIncome']:\n",
        "        values = []\n",
        "        indicies = []\n",
        "\n",
        "        for fact in xbrl_json['StatementsOfIncome'][usGaapItem]:\n",
        "            # only consider items without segment. not required for our analysis.\n",
        "            if 'segment' not in fact:\n",
        "                index = fact['period']['startDate'] + '-' + fact['period']['endDate']\n",
        "                # ensure no index duplicates are created\n",
        "                if index not in indicies:\n",
        "                    values.append(fact['value'])\n",
        "                    indicies.append(index)\n",
        "\n",
        "        income_statement_store[usGaapItem] = pd.Series(values, index=indicies)\n",
        "\n",
        "    income_statement = pd.DataFrame(income_statement_store)\n",
        "    # switch columns and rows so that US GAAP items are rows and each column header represents a date range\n",
        "    return income_statement.T\n",
        "\n",
        "file_list = []\n",
        "for root, dir, file in os.walk('/content/sec_filings/sec-edgar-filings'):\n",
        "  if len(dir) == 4:\n",
        "    dir = sorted(dir)\n",
        "    stock_name = tickers[root.split('/')[4]]\n",
        "\n",
        "    xbrl_json_1 = xbrlApi.xbrl_to_json(accession_no=dir[0])\n",
        "    xbrl_json_2 = xbrlApi.xbrl_to_json(accession_no=dir[1])\n",
        "    xbrl_json_3 = xbrlApi.xbrl_to_json(accession_no=dir[2])\n",
        "    xbrl_json_4 = xbrlApi.xbrl_to_json(accession_no=dir[3])\n",
        "\n",
        "    income_statement_1 = get_income_statement(xbrl_json_1)\n",
        "    income_statement_2 = get_income_statement(xbrl_json_2)\n",
        "    income_statement_3 = get_income_statement(xbrl_json_3)\n",
        "    income_statement_4 = get_income_statement(xbrl_json_4)\n",
        "\n",
        "    income_statements_merged = pd.concat([income_statement_1,\n",
        "                                          income_statement_2,\n",
        "                                          income_statement_3,\n",
        "                                          income_statement_4], axis=1, sort=False)\n",
        "\n",
        "    duplicates = income_statements_merged.columns[income_statements_merged.columns.duplicated()]\n",
        "    income_statements_merged = income_statements_merged.loc[:, ~income_statements_merged.columns.duplicated()]\n",
        "    income_statements = income_statements_merged.reindex(sorted(income_statements_merged.columns), axis=1)\n",
        "\n",
        "    income_statements.to_csv(f'sec_filings/{stock_name}_income_statements.csv')\n",
        "    print(f\"Parsed income statement for {stock_name}\")\n",
        "\n",
        "print(\"All statements were successfully parsed\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GshIOxzu4XOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Delete existing qdrant collection(s) if required**"
      ],
      "metadata": {
        "id": "u9SGmqFIG2g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import CollectionDescription\n",
        "from google.colab import userdata\n",
        "\n",
        "# Replace with your Qdrant Cloud endpoint and API key\n",
        "QDRANT_URL = userdata.get('QDRANT_URL')\n",
        "API_KEY = userdata.get('QDRANT_API_KEY')\n",
        "\n",
        "# Connect to Qdrant Cloud\n",
        "client = QdrantClient(\n",
        "    url=QDRANT_URL,\n",
        "    api_key=API_KEY,\n",
        ")\n",
        "\n",
        "# Get all collections\n",
        "collections = client.get_collections().collections  # List[CollectionDescription]\n",
        "\n",
        "# Delete each collection\n",
        "for collection in collections:\n",
        "    name = collection.name\n",
        "    print(f\"Deleting collection: {name}\")\n",
        "    client.delete_collection(collection_name=name)\n",
        "\n",
        "print(\"‚úÖ All collections deleted.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YZ6VVt9Mompm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create vector using the downloaded data"
      ],
      "metadata": {
        "id": "6JqzDMjco3x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client.models import PointStruct, VectorParams, Distance, PayloadSchemaType#, PayloadIndexParams\n",
        "\n",
        "# --- Qdrant config ---\n",
        "QDRANT_URL = userdata.get('QDRANT_URL')\n",
        "QDRANT_API_KEY = userdata.get('QDRANT_API_KEY')\n",
        "COLLECTION_NAME = \"Company_Finances\"\n",
        "\n",
        "# üîπ Metric descriptions dictionary\n",
        "metric_descriptions = {\n",
        "    \"RevenueFromContractWithCustomerExcludingAssessedTax\": \"Revenue from contracts with customers\",\n",
        "    \"CostOfRevenue\": \"Cost of producing and delivering products\",\n",
        "    \"ResearchAndDevelopmentExpense\": \"Expenses for research and development\",\n",
        "    \"SellingAndMarketingExpense\": \"Expenses related to selling and marketing\",\n",
        "    \"GeneralAndAdministrativeExpense\": \"General and administrative overhead\",\n",
        "    \"CostsAndExpenses\": \"Total operating costs and expenses\",\n",
        "    \"OperatingIncomeLoss\": \"Net operating income or loss\",\n",
        "    \"NonoperatingIncomeExpense\": \"Non-operating income or expense\",\n",
        "    \"IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest\": \"Income before taxes from continuing operations\",\n",
        "    \"IncomeTaxExpenseBenefit\": \"Taxes paid or refunded\",\n",
        "    \"NetIncomeLoss\": \"Net income after taxes\",\n",
        "    \"EarningsPerShareBasic\": \"Earnings per share (basic)\",\n",
        "    \"EarningsPerShareDiluted\": \"Earnings per share (diluted)\"\n",
        "}\n",
        "\n",
        "# üîπ Load the BGE Large embedding model\n",
        "model = SentenceTransformer(\"BAAI/bge-large-en\")\n",
        "\n",
        "# üîπ Connect to Qdrant\n",
        "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
        "\n",
        "# üîπ Embed with instruction prompt\n",
        "def embed(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    prompt_texts = [f\"Represent this sentence for searching relevant passages: {t}\" for t in texts]\n",
        "    return model.encode(prompt_texts, normalize_embeddings=True)\n",
        "\n",
        "# üîπ Create (or recreate) the Qdrant collection\n",
        "if client.collection_exists(collection_name=COLLECTION_NAME):\n",
        "    try:\n",
        "        client.delete_collection(collection_name=COLLECTION_NAME)\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting collection: {e}\")\n",
        "        sys.exit(\"Execution stopped: Vector deletion failed\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    vectors_config=VectorParams(\n",
        "        size=1024,\n",
        "        distance=Distance.COSINE\n",
        "    )\n",
        ")\n",
        "\n",
        "# ‚úÖ Add index for filtering\n",
        "client.create_payload_index(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    field_name=\"company\",\n",
        "    field_schema=PayloadSchemaType.KEYWORD\n",
        ")\n",
        "\n",
        "print(f\"Collection '{COLLECTION_NAME}' created.\")\n",
        "\n",
        "# üîπ Embed descriptions and upload\n",
        "descriptions = list(metric_descriptions.values())\n",
        "embeddings = embed(descriptions)\n",
        "\n",
        "income_statement = None\n",
        "for file in os.listdir('/content/sec_filings'):\n",
        "    if file.endswith('.csv'):\n",
        "        income_statement = pd.read_csv(os.path.join('/content/sec_filings', file))\n",
        "        income_statement.set_index(income_statement.columns[0], inplace=True)\n",
        "        for col in income_statement.columns:\n",
        "          income_statement[col] = income_statement[col].astype(str).str.replace('[\\$,]', '', regex=True)\n",
        "          try:\n",
        "              income_statement[col] = pd.to_numeric(income_statement[col])\n",
        "          except Exception:\n",
        "              pass\n",
        "\n",
        "        points = []\n",
        "        for i, (metric, desc) in enumerate(metric_descriptions.items()):\n",
        "            if metric not in income_statement.index:\n",
        "                # print(f\"Continued for {file}\")\n",
        "                continue\n",
        "\n",
        "            # point_id = f\"{file.split('_')[0]}_{i}\"\n",
        "            point_id = int(hash(f\"{file.split('_')[0]}_{metric}\") % (10 ** 8))\n",
        "            vector = embeddings[i]\n",
        "            values = income_statement.loc[metric].to_dict()\n",
        "            points.append(\n",
        "                PointStruct(\n",
        "                    id=point_id,\n",
        "                    vector=vector,\n",
        "                    payload={\n",
        "                        \"company\": file.split('_')[0],\n",
        "                        \"metric\": metric,\n",
        "                        \"description\": desc,\n",
        "                        \"values\": values\n",
        "                    }\n",
        "                )\n",
        "            )\n",
        "\n",
        "        if points:\n",
        "          client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
        "          print(f\"‚úÖ Uploaded {len(points)} metrics to Qdrant for {file.split('_')[0]}.\")\n",
        "        else:\n",
        "          print(f\"No points to upload for {file}\")\n",
        "\n",
        "print(\"All dataframes were successfully parsed\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "G7Ft4UMqHGC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get vector output based on the query"
      ],
      "metadata": {
        "id": "TuKECsuFo-QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from google.colab import userdata\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
        "\n",
        "# === Qdrant Configuration ===\n",
        "QDRANT_URL = userdata.get('QDRANT_URL')\n",
        "QDRANT_API_KEY = userdata.get('QDRANT_API_KEY')\n",
        "COLLECTION_NAME = \"Company_Finances\"\n",
        "\n",
        "# === Initialize Qdrant Client ===\n",
        "client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)\n",
        "\n",
        "# === Load embedding model ===\n",
        "model = SentenceTransformer(\"BAAI/bge-large-en\")\n",
        "\n",
        "def embed(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    prompts = [f\"Represent this sentence for searching relevant passages: {t}\" for t in texts]\n",
        "    return model.encode(prompts, normalize_embeddings=True)\n",
        "\n",
        "# === Extract years from query (e.g., 2021, 2022) ===\n",
        "def extract_years_from_query(query):\n",
        "    return re.findall(r\"\\b(20[1-2][0-9]|202[0-5])\\b\", query)\n",
        "\n",
        "# === Extract company from query based on known list ===\n",
        "def extract_company_from_query(query, company_list):\n",
        "    query_lower = query.lower()\n",
        "    for company in company_list:\n",
        "        if company.lower() in query_lower:\n",
        "            return company\n",
        "    return None\n",
        "\n",
        "# === Main function to search Qdrant ===\n",
        "def search_financials(query, known_companies=None):\n",
        "    query_vector = embed(query)[0]\n",
        "    filter_years = extract_years_from_query(query)\n",
        "    filter_company = extract_company_from_query(query, known_companies or [])\n",
        "\n",
        "    conditions = []\n",
        "    if filter_company:\n",
        "        conditions.append(FieldCondition(key=\"company\", match=MatchValue(value=filter_company)))\n",
        "\n",
        "    search_filter = Filter(must=conditions) if conditions else None\n",
        "\n",
        "    response = client.search(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        query_vector=query_vector,\n",
        "        limit=5,\n",
        "        query_filter=search_filter\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüîç Results for: '{query}'\")\n",
        "    if filter_company:\n",
        "        print(f\"üè¢ Filtered by company: {filter_company}\")\n",
        "    print(f\"üìÖ Showing: {', '.join(filter_years) if filter_years else 'All Years'}\")\n",
        "\n",
        "    for result in response:\n",
        "        payload = result.payload\n",
        "        metric = payload.get(\"metric\", \"N/A\")\n",
        "        description = payload.get(\"description\", \"N/A\")\n",
        "        values = payload.get(\"values\", {})\n",
        "\n",
        "        print(f\"\\n‚Ä¢ {metric} ‚Äî {description} (score: {result.score:.4f})\")\n",
        "        for year, val in values.items():\n",
        "            if not filter_years or any(y in year for y in filter_years):\n",
        "                formatted_val = f\"${val:,.2f}\" if isinstance(val, (int, float)) else val\n",
        "                print(f\"   - {year}: {formatted_val}\")\n",
        "\n",
        "# === Example ===\n",
        "known_companies = [\"Apple\",\"Microsoft\", \"Amazon\", \"Google\", \"Meta\", \"Berkshire\", \"Johnson\", \"Visa\", \"ProcterGamble\", \"Tesla\"]\n",
        "search_financials(query=\"Which company had highest profit in 2021\", known_companies=known_companies)\n"
      ],
      "metadata": {
        "id": "yX4Exr3SpNOe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
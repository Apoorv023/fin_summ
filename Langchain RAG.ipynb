{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPE3V1TZTHzzwA/Dar2RHC7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q1AsA3_RPHkw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Install libraries\n",
        "\n",
        "!pip install -q sec_edgar_downloader==5.0.3 faiss-cpu==1.11.0.post1 langchain==0.3.27 langchain-community==0.3.27 langchain-cohere==0.4.4 cohere==5.15.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import logging\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from sec_edgar_downloader import Downloader\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# LangChain & LangChain Community\n",
        "import faiss\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Cohere integrations\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain_cohere import CohereEmbeddings"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D_NkVzSVPVfk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set-up variables and files for notebook\n",
        "\n",
        "# Set up header for downloading SEC data\n",
        "headers = {\"User-Agent\": \"YourName YourEmail@example.com\"}\n",
        "\n",
        "# Clear existing handler for smooth log file creation\n",
        "for handler in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(handler)\n",
        "\n",
        "# Set up logging configuration for excel file downloads\n",
        "log_file = '/content/download_excels.log'\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=log_file,\n",
        "    filemode='a',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(message)s'\n",
        ")\n",
        "\n",
        "if not os.path.exists(log_file):\n",
        "    print(\"Log file creation failed!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uuoKIn6v6Dbf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Get accession numbers for downloading files and download files -- fetch_accession_numbers(cik) & get_excel(acc)\n",
        "\n",
        "def fetch_accession_numbers(cik):\n",
        "    try:\n",
        "        url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    filings = data.get('filings', {}).get('recent', {})\n",
        "    acc_numbers = filings.get('accessionNumber', [])\n",
        "    form_types = filings.get('form', [])\n",
        "\n",
        "    return sorted((acc for acc, form in zip(acc_numbers, form_types) if form == \"10-K\"))\n",
        "\n",
        "\n",
        "def get_excel(company, cik, accession_num):\n",
        "\n",
        "    file_name = f\"{company}_{accession_num}.xlsx\"\n",
        "    acc_clean = accession_num.replace(\"-\", \"\")\n",
        "    url = f\"https://www.sec.gov/Archives/edgar/data/{cik}/{acc_clean}/Financial_Report.xlsx\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            with open(file_name, 'wb') as f:\n",
        "                f.write(response.content)\n",
        "            return (\"success\", accession_num)\n",
        "        elif response.status_code == 404:\n",
        "            return (\"not_found\", accession_num)\n",
        "        else:\n",
        "            return (\"error\", accession_num, f\"Status code: {response.status_code}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        return (\"error\", accession_num, str(e))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gwxFkBGIj70P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download excel files using CIKs -- download_excels(company, cik)\n",
        "\n",
        "def download_excels(company, cik):\n",
        "\n",
        "    # Improve company name for better file handling\n",
        "    company = re.sub(r'[^\\w\\-_]', '_', company).strip('_')\n",
        "\n",
        "    # Fetch 10-K accession numbers from cik\n",
        "    accession_numbers = fetch_accession_numbers(cik)\n",
        "    if not accession_numbers:\n",
        "        logging.error(f\"{company}, {cik}, , Error, No 10-K accessions found\")\n",
        "        return\n",
        "\n",
        "    # To keep track of failed and successful downloads\n",
        "    failed_downloads, success_downloads = [], []\n",
        "\n",
        "    for acc in accession_numbers:\n",
        "\n",
        "        # To track number of tries in case of 429 error status\n",
        "        attempt = 1\n",
        "\n",
        "        # This loop tracks number of tries\n",
        "        while attempt < 6:\n",
        "\n",
        "            result = get_excel(company, cik, acc)\n",
        "            status, acc_num = result[0], result[1]\n",
        "\n",
        "            if status == \"success\":\n",
        "                success_downloads.append([company, cik, acc_num, \"success\", \"\"])\n",
        "                break\n",
        "\n",
        "            elif status == \"not_found\":\n",
        "                failed_downloads.append([company, cik, acc_num, \"not_found\", \"\"])\n",
        "                break\n",
        "\n",
        "            elif result[2] == \"Status code: 429\":\n",
        "                print(f\"TRY NUMBER: {attempt + 1} for {company.upper()}\")\n",
        "\n",
        "                time.sleep(75 * attempt)\n",
        "                attempt += 1\n",
        "\n",
        "                if attempt > 5:\n",
        "                    failed_downloads.append([company, cik, acc_num, \"denied\", result[2]])\n",
        "\n",
        "            else:\n",
        "                failed_downloads.append([company, cik, acc_num, \"error\", result[2]])\n",
        "                break\n",
        "\n",
        "    # Log entries in the log file\n",
        "    for entry in success_downloads:\n",
        "        logging.info(f\"{entry[0]}, {entry[1]}, {entry[2]}, {entry[3]}, {entry[4]}\")\n",
        "\n",
        "    for entry in failed_downloads:\n",
        "        logging.error(f\"{entry[0]}, {entry[1]}, {entry[2]}, {entry[3]}, {entry[4]}\")\n",
        "\n",
        "    print(f\"{company} -- Successful downloads: {len(success_downloads)} | Failed downloads: {len(failed_downloads)}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "TakNORswcVJD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Correct the values in dataframe according to share and usd multipliers -- process_df_values(df, share_multiplier, usd_multiplier)\n",
        "\n",
        "def process_df_values(df: pd.DataFrame, share_multiplier: int, usd_multiplier: int) -> pd.DataFrame:\n",
        "    pre_string = ''\n",
        "    nan_index, text_index = [], []\n",
        "\n",
        "    # Some common terms to be taken care of when converting dataframe values\n",
        "    common_averages = [\"in share\", \"average share\", \"average common share\"]\n",
        "    usd_terms = [\"in usd\", \"in dollars\", \"income per share\"]\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        try:\n",
        "            # Get row description for appropriate value conversion\n",
        "            row_desc = row.loc['Description'].lower().rstrip(':')\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        if row.isnull().all():\n",
        "          pre_string = ''\n",
        "          nan_index.append(index)\n",
        "\n",
        "        elif row.iloc[2:].isnull().all():\n",
        "          if index - 1 in nan_index:\n",
        "            pre_string = pre_string + \" | \" + row_desc\n",
        "          else:\n",
        "            pre_string = row_desc\n",
        "          nan_index.append(index)\n",
        "\n",
        "        else:\n",
        "            try:\n",
        "                # If value can't be converted to float, then skip that row\n",
        "                float(row.iloc[2])\n",
        "\n",
        "                if pre_string:\n",
        "                    df.iloc[index, 0] = pre_string\n",
        "\n",
        "                for col in df.columns[2:]:\n",
        "                    val = row[col]\n",
        "\n",
        "                    if any(term in row_desc for term in common_averages) or any(term in pre_string for term in common_averages):\n",
        "                        val = float(val) * share_multiplier\n",
        "                        val = str(val) + \" shares\"\n",
        "                    else:\n",
        "                        if all(term not in row_desc and term not in pre_string for term in usd_terms):\n",
        "                            val = float(val) * usd_multiplier\n",
        "                            if abs(val) >= 1000000000:\n",
        "                                val = str(val/1000000000) + \" billion\"\n",
        "                            elif abs(val) >= 1000000:\n",
        "                                val = str(val/1000000) + \" million\"\n",
        "\n",
        "                        val = \"USD \" + str(val)\n",
        "\n",
        "                    df.at[index, col] = val\n",
        "            except:\n",
        "                text_index.append(index)\n",
        "                continue\n",
        "\n",
        "    # Drop rows that contain text and not numbers\n",
        "    if text_index:\n",
        "      df = df.drop(text_index)\n",
        "\n",
        "    # Drop empty rows\n",
        "    df = df.dropna(subset=df.columns[1:])\n",
        "\n",
        "    return df.reset_index(drop = True)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EoYwjqJpJI_n"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set column headings and find share and usd multipliers -- process_dataframe(df)\n",
        "\n",
        "def process_dataframe(df: pd.DataFrame):\n",
        "\n",
        "    # Identify multipliers from the first row of the sheet\n",
        "    multiplier_identifier = df.columns[0].lower()\n",
        "\n",
        "    share_multiplier = 1\n",
        "    if 'shares in billion' in multiplier_identifier:\n",
        "        share_multiplier = 1000000000\n",
        "    elif 'shares in million' in multiplier_identifier:\n",
        "        share_multiplier = 1000000\n",
        "    elif 'shares in thousand' in multiplier_identifier:\n",
        "        share_multiplier = 1000\n",
        "\n",
        "    usd_multiplier = 1\n",
        "    if '$ in billion' in multiplier_identifier:\n",
        "        usd_multiplier = 1000000000\n",
        "    elif '$ in million' in multiplier_identifier:\n",
        "        usd_multiplier = 1000000\n",
        "    elif '$ in thousand' in multiplier_identifier:\n",
        "        usd_multiplier = 1000\n",
        "\n",
        "    # Rename columns with None heading for proper column renaming\n",
        "    df.columns = [f'new_column_{i}' if col is None else col for i, col in enumerate(df.columns)]\n",
        "\n",
        "    # Replace empty dataframe values with NaN for better processing\n",
        "    df = df.map(lambda x: np.nan if (x == '' or x is None or (isinstance(x, str) and x.strip() == \"\")) else x)\n",
        "\n",
        "    # Delete columns with very less unique values as they do not contain good data\n",
        "    low_unique_cols = [col for col in df.columns if df[col].nunique(dropna=True) < 5]\n",
        "    df = df.drop(columns=low_unique_cols)\n",
        "\n",
        "    # Rename columns with their respective years\n",
        "    try:\n",
        "        df = df.rename(columns = {df.columns[0]: \"Description\",\n",
        "                                  df.columns[1]: int(df.iloc[0,1][-4:]),\n",
        "                                  df.columns[2]: int(df.iloc[0,2][-4:]),\n",
        "                                  df.columns[3]: int(df.iloc[0,3][-4:])})\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    # Remove first row as its not required and add a new column 'Category' to dataframe\n",
        "    df = df.iloc[1:].reset_index(drop=True)\n",
        "    df.insert(0, 'Category', '')\n",
        "\n",
        "    return process_df_values(df, share_multiplier, usd_multiplier)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eeWTVKfzJDl-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convert excel data to dataframe -- excel_to_df(excel_file_path)\n",
        "\n",
        "def excel_to_df(excel_file_path: str):\n",
        "\n",
        "    # Description of sheet names that need to be read from excel files\n",
        "    sheet_keywords = [\n",
        "        'consolidated statements of oper',\n",
        "        'consolidated statements of inco',\n",
        "        'consolidated statement of incom',\n",
        "        'income statements',\n",
        "        'consolidated statements of earn',\n",
        "        'consolidated income statement',\n",
        "        'statement of earnings (loss)',\n",
        "        'consolidated statement of earni',\n",
        "        'statements of consolidated earn',\n",
        "        'consolidated statements of comp',\n",
        "        'statements of consolidated inco',\n",
        "        'statement of consolidated opera',\n",
        "        'statements of consolidated oper',\n",
        "        'consolidated statement of opera',\n",
        "        'statements_of_operations',\n",
        "        'consolidated results of operati',\n",
        "        'consolidated and combined state',\n",
        "        'statement_of_income',\n",
        "        'statement of income',\n",
        "        'statements of operations'\n",
        "        'income_statements'                 # For Ishares_gold_trust\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        excel_content = pd.ExcelFile(excel_file_path, engine='openpyxl')\n",
        "\n",
        "        # Find the first sheet name matching any keyword (case-insensitive)\n",
        "        matched_sheet = next(\n",
        "            (sheet for sheet in excel_content.sheet_names\n",
        "             if any(keyword in sheet.lower() for keyword in sheet_keywords)),\n",
        "            None\n",
        "        )\n",
        "\n",
        "        if not matched_sheet:\n",
        "            return f\"No matching sheet found in {excel_file_path}.\"\n",
        "\n",
        "        df = pd.read_excel(excel_file_path, sheet_name=matched_sheet, engine='openpyxl')\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Could not read the excel file {excel_file_path}: {e}\"\n",
        "\n",
        "    return process_dataframe(df)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DVzxi3hgINj-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Convert dataframe to langchain Documents -- df_to_document(df, company_name)\n",
        "\n",
        "def df_to_document(df, company_name):\n",
        "    documents = []\n",
        "\n",
        "    # Convert each dataframe row to text for mebedding\n",
        "    for _, row in df.iterrows():\n",
        "      if row.iloc[0]:\n",
        "        row_desc = f\"For {company_name} {row.iloc[1]} in {row.iloc[0]} category for year\"\n",
        "      else:\n",
        "        row_desc = f\"For {company_name} {row.iloc[1]} for year\"\n",
        "\n",
        "      for column in df.columns[2:]:\n",
        "        metadata = {\"company\": company_name, \"year\": column}\n",
        "        documents.append(Document(page_content = f\"{row_desc} {column} is {row[column]}.\", metadata = metadata))\n",
        "\n",
        "    return documents\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6q7NnbRKWcTw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Remove duplicate years between dataframes to reduce redundancy -- sync_years(df, years)\n",
        "\n",
        "def sync_years(df, years):\n",
        "    drop_cols = []\n",
        "\n",
        "    for column in df.columns[2:]:\n",
        "        try:\n",
        "          year = int(column)\n",
        "        except:\n",
        "          drop_cols.append(column)\n",
        "          continue\n",
        "\n",
        "        if year in years:\n",
        "            drop_cols.append(column)\n",
        "        else:\n",
        "            years.append(year)\n",
        "\n",
        "    return [df.drop(columns = drop_cols), years]\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "egkKq3NMa0Nt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title MAIN\n",
        "\n",
        "# Get tickers of SEC companies\n",
        "\n",
        "response = requests.get(\"https://www.sec.gov/files/company_tickers.json\", headers=headers)\n",
        "response.raise_for_status()\n",
        "tickers = tuple(response.json().values())\n",
        "\n",
        "# Fetch required number companies for which data needs to be downloaded\n",
        "number_of_companies = 500\n",
        "sec_top_companies = tickers[:number_of_companies]\n",
        "\n",
        "cohere_api_key = userdata.get('COHERE_API_KEY3')\n",
        "embeddings = CohereEmbeddings(cohere_api_key=cohere_api_key, model=\"embed-english-v3.0\")\n",
        "vectorstore = FAISS.from_documents([Document(page_content=\"dummy_data\")], embeddings)\n",
        "\n",
        "file_success = \"Success companies.txt\"\n",
        "file_failed = \"Failed companies.txt\"\n",
        "\n",
        "docs = []\n",
        "for index, ticker_desc in enumerate(sec_top_companies):\n",
        "    download_excels(ticker_desc['title'], str(ticker_desc['cik_str']).zfill(10))\n",
        "\n",
        "    # Get all files in the current directory\n",
        "    files = os.listdir()\n",
        "\n",
        "    years = []\n",
        "    for file in files:\n",
        "        if file.endswith('.xlsx') or file.endswith('.xls'):\n",
        "            df = excel_to_df(excel_file_path = file)\n",
        "            os.remove(file)\n",
        "\n",
        "            # Check if df is indeed a dataframe\n",
        "            if isinstance(df, pd.DataFrame):\n",
        "                df, years = sync_years(df, years)\n",
        "                docs.extend(df_to_document(df = df, company_name = ticker_desc['title']))\n",
        "\n",
        "    # Save company name - total files in a file\n",
        "    if years:\n",
        "        with open(file_success, 'a') as f:\n",
        "            f.write(f\"{ticker_desc['title']} - {len(files)} - {sorted(years)}\\n\")\n",
        "    else:\n",
        "        with open(file_failed, 'a') as f:\n",
        "            f.write(f\"{ticker_desc['title']} - {len(files)}\\n\")\n",
        "\n",
        "    # Embed docs for each 50 companies to reduce API requests\n",
        "    if index % 50 == 0 and docs:\n",
        "        vectorstore.add_documents(docs)\n",
        "        docs = []\n",
        "\n",
        "# LLM Chain Creation\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "llm = ChatCohere(cohere_api_key=cohere_api_key)\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=False)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DuZ5Pt9hWXrs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the vectorstore locally in Colab\n",
        "vectorstore.save_local(\"faiss_index\")"
      ],
      "metadata": {
        "id": "yJbJRuNY1UVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query to Cohere\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "llm = ChatCohere(cohere_api_key=cohere_api_key)\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=False)\n",
        "\n",
        "query = \"Nvidia net income\"\n",
        "response = qa_chain.invoke({\"query\": query})\n",
        "\n",
        "print(\"\\nAnswer:\")\n",
        "print(response['result'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pSSqliZtZ9UN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}